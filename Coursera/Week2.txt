DATA WRANGLING
Introduction to Data Preprocessing
Let us go through some data pre-processing techniques. If you're unfamiliar with the term, data pre-processing is a necessary step in data analysis. 
It is the process of converting, or mapping data from one raw form into another format, to make it ready for further analysis. 
Data pre-processing is often called data cleaning or data wrangling, and there are likely other terms. Here are the topics that we'll be covering in this module. 
First, we'll show you how to identify and handle missing values. A missing value condition occurs whenever a data entry is left empty. Then we'll cover data formats.
Data from different sources maybe in various formats, and different units or in various conventions. We will introduce some methods in Python pandas that can standardize the values 
into the same format, or unit, or convention. After that, we'll cover data normalization. 
Different columns of numerical data, may have very different ranges, and direct comparison is often not meaningful. 
Normalization is a way to bring all data into a similar range for more useful comparison. Specifically, we'll focus on the techniques of centering and scaling. 
And then will introduce data binning. Binning creates bigger categories from a set of numerical values. It is particularly useful for comparison between groups of data.
And lastly, we'll talk about categorical variables, and show you how to convert categorical values into numeric variables to make statistical modeling easier. 
In Python, we usually perform operations along columns. Each row of the column represents a sample, i.e a different used car in the database. 
You access a column by specifying the name of the column. For example, you can access symboling and body style, each of these columns is a pandas series. 
There are many ways to manipulate data frames in Python. For example, you can add a value to each entry of a column. To add one to each symboling entry, use this command. 
This changes each value of the data frame column by adding one to the current value.

Deal with Missing Values
we will introduce the pervasive problem of missing values as well as strategies on what to do when you encounter missing values in your data. 
When no data value is stored for feature for a particular observation, we say this feature has a missing value. 
Usually missing value in data set appears as question mark and a zero or just a blank cell. In the example here, the normalized losses feature has a missing value which is represented with NaN. 
But how can you deal with missing data? There are many ways to deal with missing values and this is regardless of Python, R or whatever tool you use. Of course, each situation is different 
and should be judged differently. However, these are the typical options you can consider. 
The first is to check if the person or group that collected the data can go back and find what the actual value should be. 
Another possibility is just to remove the data where that missing value is found. When you drop data, you could either drop the whole variable or just the single data entry with the 
missing value. If you don't have a lot of observations with missing data, usually dropping the particular entry is the best. 
If you're removing data, you want to look to do something that has the least amount of impact. Replacing data is better since no data is wasted. 
However, it is less accurate since we need to replace missing data with a guess of what the data should be. 
One standard for placement technique is to replace missing values by the average value of the entire variable.
As an example, suppose we have some entries that have missing values for the normalized losses column and the column average for entries with data is 4500. 
While there is no way for us to get an accurate guess of what the missing value is under the normalized losses column should have been, 
you can approximate their values using the average value of the column 4500. But what if the values cannot be averaged as with categorical variables? 
For a variable like fuel type, there isn't an average fuel type since the variable values are not numbers. In this case, one possibility is to try using the mode, 
the most common like gasoline. Finally, sometimes we may find another way to guess the missing data. 
This is usually because the data gathered knows something additional about the missing data. 
For example, he may know that the missing values tend to be old cars and the normalized losses of old cars are significantly higher than the average vehicle. 
And of course, finally, in some cases you may simply want to leave the missing data as missing data. 

Data Formatting
we'll look at the problem of data with different formats, units and conventions and the pandas methods that help us deal with these issues. 
Data is usually collected from different places by different people which may be stored in different formats. Data formatting means bringing data into a 
common standard of expression that allows users to make meaningful comparisons. As a part of dataset cleaning, data formatting ensures the data is consistent and easily understandable. 
For example, people may use different expressions to represent New York City, such as uppercase N uppercase Y, uppercase N lowercase y, uppercase N uppercase Y and New York. 
Sometimes, this unclean data is a good thing to see. For example, if you're looking at the different ways people tend to write New York, then this is exactly the data that you want. 
Or if you're looking for ways to spot fraud, perhaps writing N.Y. is more likely to predict an anomaly than if someone wrote out New York in full. 
But perhaps more often than not, we just simply want to treat them all as the same entity or format to make statistical analyses easier down the road. 
Referring to our used car dataset, there's a feature named city-miles per gallon in the dataset, which refers to a car fuel consumption in miles per gallon unit. 
However, you may be someone who lives in a country that uses metric units. So, you would want to convert those values to liters per 100 kilometers, the metric version. 
To transform miles per gallon to liters per 100 kilometers, we need to divide 235 by each value in the city-miles per gallon column. In Python, this can easily be done in one line of code. 
You take the column and set it to equal to 235, divide it by the entire column. 
In the second line of code, rename column name from city-miles per gallon to city-liters per 100 kilometers using the data frame rename method. 
For a number of reasons, including when you import a dataset into Python, the data type may be incorrectly established. 
For example, here we noticed the assigned data type to the price feature is object. Although the expected data type should really be an integer or float type. 
It is important for later analysis to explore the features data type and convert them to the correct data types. 
Otherwise, the developed models later on may behave strangely, and totally valid data may end up being treated like missing data. 
There are many data types in pandas. Objects can be letters or words. Int64 are integers and floats are real numbers. 
There are many others that we will not discuss. To identify features data type, in Python we can use the dataframe.dtypes method and check the data type of each variable in a data frame.
In the case of wrong data types, the method dataframe.astype can be used to convert a data type from one format to another. 
For example, using astype int for the price column, you can convert the object column into an integer type variable.

Data Normalization
we'll be talking about data normalization. An important technique to understand in data pre-processing.
When we take a look at the used car data set, we notice in the data that the feature length ranges from 150-250, while feature width and height ranges from 50-100. 
We may want to normalize these variables so that the range of the values is consistent. This normalization can make some statistical analyses easier down the road. 
By making the ranges consistent between variables, normalization enables a fair comparison between the different features, making sure they have the same impact. 
It is also important for computational reasons. Here is another example that will help you understand why normalization is important. 
Consider a data set containing two features, age and income. Where age ranges from 0-100, while income ranges from 0-20,000 and higher. 
Income is about 1,000 times larger than age and ranges from 20,000-500,000. So, these two features are in very different ranges. 
When we do further analysis, like linear regression for example, the attribute income will intrinsically influence the result more due to its larger value. 
But this doesn't necessarily mean it is more important as a predictor. So, the nature of the data biases the linear regression model to weigh income more heavily than age. 
To avoid this, we can normalize these two variables into values that range from zero to one. Compare the two tables at the right. 
After normalization, both variables now have a similar influence on the models we will build later. There are several ways to normalize data. I will just outline three techniques. 
The first method called simple feature scaling just divides each value by the maximum value for that feature. This makes the new values range between zero and one. 
The second method called min-max takes each value X_old subtract it from the minimum value of that feature, then divides by the range of that feature. 
Again, the resulting new values range between zero and one. The third method is called z-score or standard score. 
In this formula for each value you subtract the mu which is the average of the feature, and then divide by the standard deviation sigma. 
The resulting values hover around zero, and typically range between negative three and positive three but can be higher or lower. 
Following our earlier example, we can apply the normalization method on the length feature. 
First, we use the simple feature scaling method, where we divide it by the maximum value in the feature. Using the pandas method max, this can be done in just one line of code. 
Here's the min-max method on the length feature. We subtract each value by the minimum of that column, then divide it by the range of that column. The max minus the min. 
Finally, we apply the z-score method on length feature to normalize the values. Here we apply the mean and STD method on the length feature. 
Mean method will return the average value of the feature in the data set, and STD method will return the standard deviation of the features in the data set.

Data Binning
we'll be talking about binning as a method of data pre-processing. Binning is when you group values together into bins. 
For example, you can bin age into 0-5, 6-10, 11-15, and so on. Sometimes binning can improve accuracy of the predictive models. 
In addition, sometimes we use data binning to group a set of numerical values into a smaller number of bins to have a better understanding of the data distribution. 
As example price. Here is an attribute range from 5,000 to 45,500. Using binning we categorize the price into three bins. Low price, medium price, and high prices. 
In the actual car data set, price is numerical variable ranging from 5,188 to 45,400. It has 201 unique values. We can categorize them into three bins. 
Low, medium, and high priced cars. In Python we can easily implement the binning. First we specify that we want three bins. 
We want to divide the range of the data into three equally sized bins. But the function cut requires an extra bin, so we divide it by four. 
In the second line, we build the bin array from min value to max value using bin width calculated above. The bin array contains the information on where each bin begins and ends. 
In the third line, we create labels for each bin. Low, medium, and high. And in the last line we create a column similar to the one with the group price of a car. 
You can then use histograms to visualize the distribution of the data after they've been divided into bins. 
This is the histogram that we plotted based on the binning that we applied in the price feature. 
From the plot, it is clear that most cars have a low price and only very few cars have high price.

Categorical to Quantitative
we'll discuss how to turn categorical variables into quantitative variables in Python. 
Most statistical models cannot take in objects or strings as input and for model training only take the numbers as inputs. 
In the car data set, the fuel type feature as a categorical variable has two values, gas or diesel, which are in string format. 
For further analysis, Jerry has to convert these variables into some form of numeric format. 
We encode the values by adding new features corresponding to each unique element in the original feature we would like to encode. 
In the case where the feature fuel has two unique values, gas and diesel, we create two new features, gas and diesel. 
When a value occurs in the original feature, we set the corresponding value to one in the new feature. The rest of the features are set to zero. 
In the fuel example, for car B, the fuel value is Diesel. Therefore we set the feature diesel equal to one and the gas feature to zero. 
Similarly, for Car D, the fuel value is gas. Therefore we set the feature gas equal to one and the feature diesel equal to zero. This technique is often called "One-hot encoding". 
In Pandas, we can use get_dummies method to convert categorical variables to dummy variables. In Python, transforming categorical variables to dummy variables is simple. 
Following the example, pd.get_dummies method gets the fuel type column and creates the data frame dummy_variable_1. 
The get_dummies method automatically generates a list of numbers, each one corresponding to a particular category of the variable.